{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing, Combination and Exploratory Data Analysis of Temperature, Traffic and Stress Performance datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### The task\n",
    "\n",
    "Investigate the performance of multilayer perceptrons for prediction of a stress-based performance indicator for welded bridge joints based on temperature, traffic and strain measurements for the Great Belt Bridge (Denmark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td><img src='https://storebaelt-prod.peytz.dk/files/inline-images/luftfoto-oestbro.jpg'></td>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The necessary libraries are imported for data processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets were provided from three different sources, and represent three variables as per below: \n",
    "    1. Stress\n",
    "    2. Traffic\n",
    "    3. Temperature\n",
    "\n",
    "As with most Machine Learning (ML) models, performance is dependant on the quality of the data on which the ML models are trained.\n",
    "The data readings for the traffic, temperature and the stress performance indicator have been recorded at different sampling rates and for this reason some data munging is necessary to preprocess the data before it becomes useful for training/validation and test evaluation. The time duration is chosen to be 1 day (24 hours) and hence the three datasets are processed to reflect the chosen time discretization step Δt:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Temperature - 2012</center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Temperature_2012 = pd.read_excel('Data\\DATA Storebælt\\Temperature\\Station 9902_2012.xlsx', header=None)\n",
    "Temperature_2012.columns = [\"Average Temperature (°C)\", \"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"]\n",
    "Temperature_2012['Timestamp'] = pd.to_datetime(Temperature_2012['Year']*10000000000\n",
    "                                +Temperature_2012['Month']*100000000\n",
    "                                +Temperature_2012['Day']*1000000\n",
    "                                +Temperature_2012['Hour']*10000\n",
    "                                +Temperature_2012['Minute']*100\n",
    "                                ,format='%Y%m%d%H%M%S')\n",
    "Temperature_2012 = Temperature_2012.drop(['Year', 'Month', 'Day', 'Hour', 'Minute'], axis=1)\n",
    "Training_year_2012 = (Temperature_2012['Timestamp'] >= '2012-01-01 00:00:00') & (Temperature_2012['Timestamp'] <= '2012-12-31 23:59:59')\n",
    "Temperature_2012 = Temperature_2012.loc[Training_year_2012]\n",
    "Temperature_2012.index = Temperature_2012['Timestamp']\n",
    "Temperature_2012 = Temperature_2012.drop('Timestamp', axis=1)\n",
    "Temperature_2012 = Temperature_2012.resample('d').mean()\n",
    "\n",
    "#Temperature_2012.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Traffic - 2012</center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Traffic_2012 = pd.read_excel('Data\\DATA Storebælt\\Traffic\\Trafik_2012.xlsx')\n",
    "Traffic_2012['Timestamp'] = pd.to_datetime(Traffic_2012['Year']*10000000000\n",
    "                                +Traffic_2012['Month']*100000000\n",
    "                                +Traffic_2012['Day']*1000000\n",
    "                                +Traffic_2012['Hour']*10000\n",
    "                                ,format = '%Y%m%d%H%M%S')\n",
    "Traffic_2012.columns = ['Year', 'Month','Day', 'Hour','Class 1 - East', 'Class 2 - East','Class 3 - East', 'Class 4 - East','Class 5 - East', 'Class 6 - East','Class 7 - East', 'Class 8 - East','Class 9 - East', 'Class 10 - East','Class 1 - West', 'Class 2 - West','Class 3 - West', 'Class 4 - West','Class 5 - West', 'Class 6 - West','Class 7 - West', 'Class 8 - West','Class 9 - West', 'Class 10 - West','Total East', 'Total West', 'Total','Timestamp']\n",
    "Traffic_2012.index = Traffic_2012['Timestamp']\n",
    "Traffic_2012 = Traffic_2012.drop(['Year', 'Month','Day', 'Hour', 'Class 7 - East', 'Class 8 - East','Class 9 - East', 'Class 10 - East', 'Class 7 - West', 'Class 8 - West','Class 9 - West', 'Class 10 - West','Total East', 'Total West', 'Total'], axis=1)\n",
    "Training_year_2012 = (Traffic_2012['Timestamp'] >= '2012-01-01 00:00:00') & (Traffic_2012['Timestamp'] <= '2012-12-31 23:59:59')\n",
    "Traffic_2012 = Traffic_2012.loc[Training_year_2012]\n",
    "Traffic_2012 = Traffic_2012.resample('d').sum()\n",
    "\n",
    "#Traffic_2012.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Stress Performance Indicator - 2012</center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Stress_Indicator_2012 = pd.read_excel('Data\\DATA Storebælt\\Stress\\DD2012.xlsx', header=None)\n",
    "Stress_Indicator_2012 = Stress_Indicator_2012.T\n",
    "Stress_Indicator_2012 = Stress_Indicator_2012.append(['NaN'])\n",
    "Stress_Indicator_2012.columns = ['SG1', 'SG2','SG3','SG4','SG5','SG6','SG7','SG8','SG9','SG10','SG11','SG12','SG13','SG14','SG15']\n",
    "Stress_Indicator_2012 = Stress_Indicator_2012.drop(['SG10','SG11','SG12','SG13','SG14','SG15'], axis=1)\n",
    "Stress_Indicator_2012.index = pd.date_range('2012-01-01', '2012-12-31', freq='d')\n",
    "Stress_Indicator_2012.index.name='Timestamp'\n",
    "\n",
    "#Stress_Indicator_2012.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Merging Temperature, Traffic & Stress Performance Indicator - 2012</center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data_2012 =  pd.concat([Temperature_2012,Traffic_2012,Stress_Indicator_2012], join='inner', axis=1)\n",
    "Data_2012 = Data_2012.reset_index()\n",
    "Data_2012 = Data_2012.drop(['Timestamp'], axis=1)\n",
    "Data_2012 = Data_2012.dropna()\n",
    "\n",
    "#Data_2012.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Temperature - 2011</center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Temperature_2011 = pd.read_excel('Data\\DATA Storebælt\\Temperature\\Temperature.xlsx', header=None, skiprows=5) \n",
    "Temperature_2011 = Temperature_2011.loc[:, 0:1]\n",
    "Temperature_2011.columns = [\"Timestamp\", \"Average Temperature (°C)\"]\n",
    "Temperature_2011[\"Timestamp\"] = Temperature_2011[\"Timestamp\"].astype(str)\n",
    "Temperature_2011[\"Timestamp\"] = pd.to_datetime(Temperature_2011[\"Timestamp\"])\n",
    "Temperature_2011.index = Temperature_2011['Timestamp']\n",
    "Test_year_2011 = (Temperature_2011['Timestamp'] >= '2011-01-01 00:00:00') & (Temperature_2011['Timestamp'] <= '2011-12-31 23:59:59')\n",
    "Temperature_2011 = Temperature_2011.loc[Test_year_2011]\n",
    "Temperature_2011 = Temperature_2011.drop('Timestamp', axis=1)\n",
    "Temperature_2011 = Temperature_2011.resample('d').mean()\n",
    "\n",
    "#Temperature_2011.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Traffic - 2011</center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Traffic_2011 = pd.read_excel('Data\\DATA Storebælt\\Traffic\\Timetrafiksiden1998.xlsx')\n",
    "Traffic_2011.columns = ['Year', 'Month','Day', 'Hour','Class 1 - East', 'Class 2 - East','Class 3 - East', 'Class 4 - East','Class 5 - East', 'Class 6 - East','Class 7 - East', 'Class 8 - East','Class 9 - East', 'Class 10 - East','Class 1 - West', 'Class 2 - West','Class 3 - West', 'Class 4 - West','Class 5 - West', 'Class 6 - West','Class 7 - West', 'Class 8 - West','Class 9 - West', 'Class 10 - West','Total East', 'Total West', '2011 Total']\n",
    "Traffic_2011['Timestamp'] = pd.to_datetime(Traffic_2011['Year']*10000000000\n",
    "                                +Traffic_2011['Month']*100000000\n",
    "                                +Traffic_2011['Day']*1000000\n",
    "                                ,format = '%Y%m%d%H%M%S')\n",
    "Traffic_2011 = Traffic_2011.drop(['Year', 'Month','Day', 'Hour', 'Class 7 - East', 'Class 8 - East','Class 9 - East', 'Class 10 - East','Class 7 - West', 'Class 8 - West','Class 9 - West', 'Class 10 - West','Total East', 'Total West', '2011 Total'], axis=1)\n",
    "Traffic_2011.index = Traffic_2011['Timestamp']\n",
    "year_2011 = (Traffic_2011['Timestamp'] >= '2011-01-01 00:00:00') & (Traffic_2011['Timestamp'] <= '2011-12-31 23:59:59')\n",
    "Traffic_2011 = Traffic_2011.loc[year_2011]\n",
    "Traffic_2011 = Traffic_2011.resample('d').sum()\n",
    "\n",
    "#Traffic_2011.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Stress Performance Indicator - 2011</center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Stress_Indicator_2011 = pd.read_excel('Data\\DATA Storebælt\\Stress\\DD2011.xlsx', header=None)\n",
    "Stress_Indicator_2011 = Stress_Indicator_2011.T\n",
    "Stress_Indicator_2011.columns = ['SG1', 'SG2','SG3','SG4','SG5','SG6','SG7','SG8','SG9','SG10','SG11','SG12','SG13','SG14','SG15']\n",
    "Stress_Indicator_2011=Stress_Indicator_2011.drop(['SG10','SG11','SG12','SG13','SG14','SG15'], axis=1)\n",
    "Stress_Indicator_2011.index = pd.date_range('2011-01-01', '2011-12-31', freq='d', )\n",
    "\n",
    "#Stress_Indicator_2011.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Merging Temperature, Traffic & Stress Performance Indicator - 2011</center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data_2011 =  pd.concat([Temperature_2011,Traffic_2011,Stress_Indicator_2011], join='inner', axis=1)\n",
    "Data_2011 = Data_2011.reset_index()\n",
    "Data_2011.rename(columns={'index':'Timestamp'}, inplace=True)\n",
    "Data_2011 = Data_2011.drop(['Timestamp'], axis=1)\n",
    "Data_2011 = Data_2011.dropna()\n",
    "\n",
    "#Data_2011.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas profiling calculates summary statistics and allows for visual intepretation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install pandas_profiling\n",
    "#import pandas_profiling\n",
    "#pandas_profiling.ProfileReport(Data_2011)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixiedust provides a flexible interface to explore data graphically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "binsize": "124",
      "chartsize": "96",
      "handlerId": "barChart",
      "kde": "true",
      "keyFields": "Timestamp",
      "legend": "true",
      "lineChartType": "grouped",
      "mpld3": "false",
      "rendererId": "bokeh",
      "rowCount": "500",
      "rug": "false",
      "timeseries": "false",
      "valueFields": "SG7,SG8,SG13,SG14"
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install pixiedust\n",
    "#import pixiedust\n",
    "#display(Data_2011)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned datasets from 2011 and 2012 are merged into a single dataset and shuffled at random:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model performance metrics are defined..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Akaike Information Criterion (AIC) is defined as:\n",
    "<img src=\"AIC.jpg\" alt=\"Drawing\" style=\"width: 400px;\" position = \"centre\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIC is biased for small samples. Hence, a bias corrected version referred as AICc is preferred\n",
    "over the classical formulation, see Burnham & Anderson (2002).\n",
    "The Akaike Information Criterion (Corrected) is defined as:\n",
    "<img src=\"AICc.jpg\" alt=\"Drawing\" style=\"width: 300px;\" position = left/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Define the formula for Mean Square Error:\n",
    "<img src=\"mse.jpg\" alt=\"Drawing\" style=\"width: 320px position = \"centre\"\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the formula for Mean Absolute Percentage Error:\n",
    "<img src=\"MAPE.jpg\" alt=\"Drawing\" style=\"width: 380px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip uninstall h2o\n",
    "# Next, use pip to install this version of the H2O Python module.\n",
    "#!pip install http://h2o-release.s3.amazonaws.com/h2o/master/3978/Python/h2o-3.13.0.3978-py2.py3-none-any.whl\n",
    "\n",
    "# Relavent reading material\n",
    "# help(H2ODeepLearningEstimator)\n",
    "# help(h2o.import_file)\n",
    "# https://github.com/h2oai/h2o-tutorials/blob/master/h2o-open-tour-2016/chicago/grid-search-model-selection.ipynb\n",
    "# https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/dl/dl.md\n",
    "# https://h2o-release.s3.amazonaws.com/h2o/rel-slater/9/docs-website/h2o-docs/booklets/DeepLearning_Vignette.pdf\n",
    "# https://blog.h2o.ai/2016/06/hyperparameter-optimization-in-h2o-grid-search-random-search-and-the-future/\n",
    "# https://www.kaggle.com/faizanbatra/house-prices-advanced-regression-techniques/dm-fbatra/run/492655\n",
    "# https://github.com/h2oai/h2o-3/blob/master/h2o-py/h2o/grid/grid_search.py\n",
    "# https://blog.dominodatalab.com/deep-learning-with-h2o-ai/ - H20 easy setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Define the grid search parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H20's Deep learning library is based on a multi-layer feedforward artificial neural networks trained using stochastic gradient decent using back-propagation. Each compute node trains a copy of the global model parameters on its local data with multi-threading (asynchronously) and contributes periodically to the global model via model averaging across the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)\n",
      "  Starting server from C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\MRDEHA~1\\AppData\\Local\\Temp\\tmppdxy4e2e\n",
      "  JVM stdout: C:\\Users\\MRDEHA~1\\AppData\\Local\\Temp\\tmppdxy4e2e\\h2o_Mr_DE_Haveron_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\MRDEHA~1\\AppData\\Local\\Temp\\tmppdxy4e2e\\h2o_Mr_DE_Haveron_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.12.0.1</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>2 months and 28 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_Mr_DE_Haveron_mt6n8p</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>3.083 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.1 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------------\n",
       "H2O cluster uptime:         02 secs\n",
       "H2O cluster version:        3.12.0.1\n",
       "H2O cluster version age:    2 months and 28 days\n",
       "H2O cluster name:           H2O_from_python_Mr_DE_Haveron_mt6n8p\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    3.083 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "Python version:             3.6.1 final\n",
       "--------------------------  ------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h2o\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Define the training & cross-validation dataframe\n",
    "training_frame = Data_2012\n",
    "training_frame = h2o.H2OFrame(training_frame)\n",
    "training_frame = training_frame.drop([0], axis=0)\n",
    "\n",
    "# Define the test dataframe\n",
    "test_frame = Data_2011\n",
    "test_frame = h2o.H2OFrame(test_frame)\n",
    "test_frame = test_frame.drop([0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model SG1 & calculating training error...\n",
      "deeplearning Grid Build progress: |███████████████████████████████████████| 100%\n",
      "\n",
      "Training model SG2 & calculating training error...\n",
      "deeplearning Grid Build progress: |███████████████████████████████████████| 100%\n",
      "\n",
      "Training model SG3 & calculating training error...\n",
      "deeplearning Grid Build progress: |███████████████████████████████████████| 100%\n",
      "\n",
      "Training model SG4 & calculating training error...\n",
      "deeplearning Grid Build progress: |███████████████████████████████████████| 100%\n",
      "\n",
      "Training model SG5 & calculating training error...\n",
      "deeplearning Grid Build progress: |███████████████████████████████████████| 100%\n",
      "\n",
      "Training model SG6 & calculating training error...\n",
      "deeplearning Grid Build progress: |███████████████████████████████████████| 100%\n",
      "\n",
      "Training model SG7 & calculating training error...\n",
      "deeplearning Grid Build progress: |███████████████████████████████████████| 100%\n",
      "\n",
      "Training model SG8 & calculating training error...\n",
      "deeplearning Grid Build progress: |███████████████████████████████████████| 100%\n",
      "\n",
      "Training model SG9 & calculating training error...\n",
      "deeplearning Grid Build progress: |███████████████████████████████████████| 100%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Prepare predictors\n",
    "x = ['Average Temperature',\n",
    "    #'Class 1 - East',\n",
    "    #'Class 2 - East',\n",
    "    #'Class 3 - East',\n",
    "    #'Class 4 - East',\n",
    "    'Class 5 - East',\n",
    "    'Class 6 - East',\n",
    "    #'Class 1 - West',\n",
    "    #'Class 2 - West',\n",
    "    #'Class 3 - West',\n",
    "    #'Class 4 - West',\n",
    "    'Class 5 - West',\n",
    "    'Class 6 - West' ]\n",
    "\n",
    "m = len(x)\n",
    "\n",
    "\n",
    "# Setup the model to train 9 seperate models and calculate performance results\n",
    "my_target_variables = ['SG1','SG2','SG3','SG4','SG5','SG6','SG7','SG8','SG9']\n",
    "\n",
    "models = {}\n",
    "\n",
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n",
    "\n",
    "# Potential hyperparameters #[m+1,m+1]\n",
    "hyper_parameters = { 'activation'  : [\"Rectifier\"],\n",
    "                     'hidden' : [[m-1], [m-1,m-1], [m], [m,m], [m,m,m], [m+1,m+1,m+1], [2*m], [20], [30], [40], [50]],\n",
    "                     'input_dropout_ratio'  : [0,0.05],\n",
    "                     'l1'  : [0,1e-2, 1e-4,1e-6],\n",
    "                     'l2'  : [0,1e-2, 1e-4,1e-6]\n",
    "              \n",
    "                   }\n",
    "    \n",
    "\n",
    "search_criteria  = { 'strategy' : \"RandomDiscrete\",\n",
    "                    'stopping_metric': \"mse\",\n",
    "                     #'max_runtime_secs' : 3600, \n",
    "                     'max_models' : 6, \n",
    "                     'seed' : 12345, \n",
    "                     'stopping_rounds' : 5, \n",
    "                    }\n",
    "\n",
    "deep_learning_random_grid = H2OGridSearch(H2ODeepLearningEstimator(),hyper_parameters)\n",
    "\n",
    "for variable in my_target_variables:\n",
    "    print('Training model '+ variable + ' & calculating training error...'  )\n",
    "    deep_learning_random_grid.train(\n",
    "                                  training_frame=training_frame,\n",
    "                                  nfolds = 10, \n",
    "                                  x=x, \n",
    "                                  y=variable,\n",
    "                                  overwrite_with_best_model= True,\n",
    "                                  epochs=15,\n",
    "                                  standardize = True,\n",
    "                                  shuffle_training_data = True,\n",
    "                                  distribution=\"gaussian\",\n",
    "                                  variable_importances=True,\n",
    "                                  adaptive_rate = True,         \n",
    "                                  train_samples_per_iteration=-2,\n",
    "                                  score_validation_samples=10, \n",
    "                                  score_duty_cycle=0.025,         \n",
    "                                  max_w2=10,                  \n",
    "                                  search_criteria = search_criteria)\n",
    "    grid = deep_learning_random_grid.get_grid(sort_by='mse')\n",
    "    best_model = grid[4]\n",
    "    models[variable] =  best_model\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     activation hidden input_dropout_ratio      l1      l2  \\\n",
      "0     Rectifier   [40]                 0.0     0.0     0.0   \n",
      "1     Rectifier   [50]                0.05     0.0     0.0   \n",
      "2     Rectifier   [20]                0.05  1.0E-4  1.0E-4   \n",
      "3     Rectifier    [4]                0.05    0.01    0.01   \n",
      "4     Rectifier   [40]                0.05  1.0E-4    0.01   \n",
      "5     Rectifier   [10]                 0.0    0.01  1.0E-6   \n",
      "6     Rectifier   [40]                 0.0     0.0     0.0   \n",
      "7     Rectifier   [50]                0.05     0.0     0.0   \n",
      "8     Rectifier   [40]                0.05  1.0E-4    0.01   \n",
      "9     Rectifier   [10]                 0.0    0.01  1.0E-6   \n",
      "10    Rectifier   [20]                0.05  1.0E-4  1.0E-4   \n",
      "11    Rectifier   [40]                 0.0     0.0     0.0   \n",
      "12    Rectifier    [4]                0.05    0.01    0.01   \n",
      "13    Rectifier   [20]                0.05  1.0E-4  1.0E-4   \n",
      "14    Rectifier   [40]                0.05  1.0E-4    0.01   \n",
      "15    Rectifier   [50]                0.05     0.0     0.0   \n",
      "16    Rectifier   [50]                0.05     0.0     0.0   \n",
      "17    Rectifier   [40]                 0.0     0.0     0.0   \n",
      "18    Rectifier   [10]                 0.0    0.01  1.0E-6   \n",
      "19    Rectifier   [20]                0.05  1.0E-4  1.0E-4   \n",
      "20    Rectifier    [4]                0.05    0.01    0.01   \n",
      "21    Rectifier   [40]                0.05  1.0E-4    0.01   \n",
      "22    Rectifier   [40]                 0.0     0.0     0.0   \n",
      "23    Rectifier   [40]                0.05  1.0E-4    0.01   \n",
      "24    Rectifier   [40]                 0.0     0.0     0.0   \n",
      "25    Rectifier    [4]                0.05    0.01    0.01   \n",
      "26    Rectifier   [20]                0.05  1.0E-4  1.0E-4   \n",
      "27    Rectifier   [50]                0.05     0.0     0.0   \n",
      "28    Rectifier   [50]                0.05     0.0     0.0   \n",
      "29    Rectifier   [40]                 0.0     0.0     0.0   \n",
      "30    Rectifier    [4]                0.05    0.01    0.01   \n",
      "31    Rectifier   [10]                 0.0    0.01  1.0E-6   \n",
      "32    Rectifier   [50]                0.05     0.0     0.0   \n",
      "33    Rectifier   [10]                 0.0    0.01  1.0E-6   \n",
      "34    Rectifier   [20]                0.05  1.0E-4  1.0E-4   \n",
      "35    Rectifier    [4]                0.05    0.01    0.01   \n",
      "36    Rectifier   [40]                 0.0     0.0     0.0   \n",
      "37    Rectifier   [40]                0.05  1.0E-4    0.01   \n",
      "38    Rectifier   [20]                0.05  1.0E-4  1.0E-4   \n",
      "39    Rectifier   [40]                0.05  1.0E-4    0.01   \n",
      "40    Rectifier   [50]                0.05     0.0     0.0   \n",
      "41    Rectifier   [10]                 0.0    0.01  1.0E-6   \n",
      "42    Rectifier   [40]                0.05  1.0E-4    0.01   \n",
      "43    Rectifier   [10]                 0.0    0.01  1.0E-6   \n",
      "44    Rectifier    [4]                0.05    0.01    0.01   \n",
      "45    Rectifier    [4]                0.05    0.01    0.01   \n",
      "46    Rectifier   [20]                0.05  1.0E-4  1.0E-4   \n",
      "47    Rectifier   [10]                 0.0    0.01  1.0E-6   \n",
      "48    Rectifier   [50]                0.05     0.0     0.0   \n",
      "49    Rectifier   [40]                 0.0     0.0     0.0   \n",
      "50    Rectifier   [20]                0.05  1.0E-4  1.0E-4   \n",
      "51    Rectifier   [40]                0.05  1.0E-4    0.01   \n",
      "52    Rectifier   [10]                 0.0    0.01  1.0E-6   \n",
      "53    Rectifier    [4]                0.05    0.01    0.01   \n",
      "\n",
      "                                                                model_ids  \\\n",
      "0   Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_23   \n",
      "1   Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_20   \n",
      "2   Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_21   \n",
      "3   Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_18   \n",
      "4   Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_19   \n",
      "5   Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_22   \n",
      "6   Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_29   \n",
      "7   Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_26   \n",
      "8   Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_25   \n",
      "9   Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_28   \n",
      "10  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_27   \n",
      "11  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_17   \n",
      "12  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_12   \n",
      "13  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_15   \n",
      "14  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_13   \n",
      "15  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_14   \n",
      "16  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_50   \n",
      "17  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_53   \n",
      "18  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_16   \n",
      "19  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_51   \n",
      "20  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_48   \n",
      "21  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_49   \n",
      "22  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_35   \n",
      "23  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_31   \n",
      "24  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_11   \n",
      "25  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_24   \n",
      "26  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_33   \n",
      "27  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_32   \n",
      "28  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_38   \n",
      "29  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_41   \n",
      "30  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_30   \n",
      "31  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_52   \n",
      "32   Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_8   \n",
      "33  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_34   \n",
      "34  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_39   \n",
      "35  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_36   \n",
      "36   Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_5   \n",
      "37   Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_1   \n",
      "38   Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_3   \n",
      "39   Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_7   \n",
      "40   Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_2   \n",
      "41  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_10   \n",
      "42  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_37   \n",
      "43  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_40   \n",
      "44   Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_0   \n",
      "45   Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_6   \n",
      "46   Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_9   \n",
      "47   Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_4   \n",
      "48  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_44   \n",
      "49  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_47   \n",
      "50  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_45   \n",
      "51  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_43   \n",
      "52  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_46   \n",
      "53  Grid_DeepLearning_py_1_sid_8fab_model_python_1504542659697_1_model_42   \n",
      "\n",
      "                      mse  \n",
      "0   1.1738145245642036E11  \n",
      "1   1.3213958508289883E11  \n",
      "2   1.4611700022053094E11  \n",
      "3   1.6182352129378027E11  \n",
      "4   1.7415663576261697E11  \n",
      "5   1.8926772451030453E11  \n",
      "6    2.574700980780311E11  \n",
      "7   2.6287880127450388E11  \n",
      "8   3.0970796004247217E11  \n",
      "9   3.4767568631262634E11  \n",
      "10   4.074575885345985E11  \n",
      "11  5.0289196791936273E11  \n",
      "12   5.313491263016855E11  \n",
      "13   5.445845592941284E11  \n",
      "14   5.621920785649607E11  \n",
      "15    5.79304824030433E11  \n",
      "16   6.182260628545436E11  \n",
      "17   6.429131785913788E11  \n",
      "18   7.014263345546985E11  \n",
      "19   7.278897164259241E11  \n",
      "20   7.722018707772911E11  \n",
      "21   8.333422691236228E11  \n",
      "22   8.489860653863473E11  \n",
      "23   8.963982459090804E11  \n",
      "24   9.163391980625266E11  \n",
      "25   9.185185794887455E11  \n",
      "26   9.637577590408835E11  \n",
      "27   9.779147367687327E11  \n",
      "28   9.938141185022888E11  \n",
      "29  1.0242375039007385E12  \n",
      "30  1.0450189021435348E12  \n",
      "31  1.0690010036026327E12  \n",
      "32  1.0761130742752109E12  \n",
      "33   1.129095123121676E12  \n",
      "34   1.157886789206957E12  \n",
      "35  1.2198567355090657E12  \n",
      "36  1.2429976473254604E12  \n",
      "37   1.256477482340747E12  \n",
      "38  1.3119190837721038E12  \n",
      "39  1.3287005128576226E12  \n",
      "40  1.3372291763596086E12  \n",
      "41  1.3464571164008801E12  \n",
      "42  1.3912457359513728E12  \n",
      "43  1.4048940580475059E12  \n",
      "44   1.443620130448209E12  \n",
      "45   1.485627185946438E12  \n",
      "46  1.5421119251775852E12  \n",
      "47    1.81352800810827E12  \n",
      "48  2.4514529870635195E12  \n",
      "49   2.769049536325548E12  \n",
      "50  3.0756351874920234E12  \n",
      "51   3.601145371467539E12  \n",
      "52  3.6530302574643174E12  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53  3.7077035506601826E12  \n"
     ]
    }
   ],
   "source": [
    "grid.show()\n",
    "#best_model.get_hyperparams_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def evaluate_model(y_observed, y_predicted, m):\n",
    "    \n",
    "    # Ensure the vectors are of equal lengthzz\n",
    "    assert len(y_observed) == len(y_predicted)\n",
    "    #y_actual, y_predicted = check_array(y_actual, y_predicted)\n",
    "\n",
    "    # Calculate the Mean Square Error (MSE)\n",
    "    #MSE = np.mean((y_observed-y_predicted)**2) or MSE = mean_squared_error(y_actual, y_predicted) - Cross Validation is used\n",
    "    \n",
    "    #######################################################\n",
    "    \n",
    "    # Calculate the Mean Absolute Percentage Error (MAPE)\n",
    "    MAPE = np.mean(np.abs((y_observed - y_predicted) / y_observed)) * 100\n",
    "    MAPE = \"{0:0.1f}\".format(MAPE)\n",
    "    \n",
    "    #######################################################\n",
    "    \n",
    "    # Calculate the AIC & AICc\n",
    "    # r is number of features\n",
    "    n =  len(y_observed) #number of observations\n",
    "\n",
    "    resid = y_observed - y_predicted\n",
    "    sse = sum(resid ** 2)\n",
    "\n",
    "    AIC =  2*m - 2*np.log(sse)\n",
    "    AICc = AIC + (2*m*(m+1))/(n-m-1)\n",
    "    AICc = float(AICc)\n",
    "    AICc = \"{0:0.1f}\".format(AICc)\n",
    "    \n",
    "    #######################################################\n",
    "    \n",
    "    # Calculate the Ψ value\n",
    "    Ψ = sum(y_predicted)/sum(y_observed)\n",
    "    Ψ = float(Ψ)*100\n",
    "    Ψ = \"{0:0.1f}\".format(Ψ)\n",
    "\n",
    "    return MAPE, AICc, Ψ\n",
    "\n",
    "def access_dictionary(dictionary_name, *keys):\n",
    "    for key in keys:\n",
    "        try:\n",
    "            dictionary_name = dictionary_name[key]\n",
    "        except KeyError:\n",
    "            return None\n",
    "    return dictionary_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplearning prediction progress: |███████████████████████████████████████| 100%\n",
      "deeplearning prediction progress: |███████████████████████████████████████| 100%\n",
      "deeplearning prediction progress: |███████████████████████████████████████| 100%\n",
      "deeplearning prediction progress: |███████████████████████████████████████| 100%\n",
      "deeplearning prediction progress: |███████████████████████████████████████| 100%\n",
      "deeplearning prediction progress: |███████████████████████████████████████| 100%\n",
      "deeplearning prediction progress: |███████████████████████████████████████| 100%\n",
      "deeplearning prediction progress: |███████████████████████████████████████| 100%\n",
      "deeplearning prediction progress: |███████████████████████████████████████| 100%\n",
      "deeplearning prediction progress: |███████████████████████████████████████| 100%\n",
      "deeplearning prediction progress: |███████████████████████████████████████| 100%\n",
      "deeplearning prediction progress: |███████████████████████████████████████| 100%\n",
      "deeplearning prediction progress: |███████████████████████████████████████| 100%\n",
      "deeplearning prediction progress: |███████████████████████████████████████| 100%\n",
      "deeplearning prediction progress: |███████████████████████████████████████| 100%\n",
      "deeplearning prediction progress: |███████████████████████████████████████| 100%\n",
      "deeplearning prediction progress: |███████████████████████████████████████| 100%\n",
      "deeplearning prediction progress: |███████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Define the y_observed & y_predicted for training/test error calculation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "model_results = {}\n",
    "\n",
    "def calculate_model_performance(models):\n",
    "    my_target_variables = ['SG1','SG2','SG3','SG4','SG5','SG6','SG7','SG8','SG9']\n",
    "    for variable in my_target_variables:\n",
    " \n",
    "#TRAINING PERFORMANCE\n",
    "        # Define Training y_observed\n",
    "        y_obs_train = training_frame[variable].as_data_frame(use_pandas=True) # y_observed - training, convert to pandas df\n",
    "        y_obs_train = y_obs_train.as_matrix() # convert to numpy array\n",
    "        y_pred_train = models[variable].predict(training_frame).as_data_frame(use_pandas=True) # y_predicted - training\n",
    "        y_pred_train = y_pred_train.as_matrix()\n",
    "        \n",
    "        # Call the Cross_validated MSE values for each Strain Gauge\n",
    "        MSE_train = models[variable].mse(xval=True) # Cross validation mse calucluated\n",
    "        MSE_train = \"{:.2E}\".format(MSE_train)\n",
    "        \n",
    "        # Call the evaluate_model function for each Strain Gauge\n",
    "        MAPE_train, AICc_train, Ψ_train  = evaluate_model(y_obs_train, y_pred_train, m)\n",
    "        \n",
    "#TEST PERFORMANCE\n",
    "        # Define Training y_observed\n",
    "        y_obs_test = test_frame[variable].as_data_frame(use_pandas=True) # y_observed - training, convert to pandas df\n",
    "        y_obs_test = y_obs_test.as_matrix() # convert to numpy array\n",
    "        y_pred_test = models[variable].predict(test_frame).as_data_frame(use_pandas=True) # y_predicted - training\n",
    "        y_pred_test = y_pred_test.as_matrix()\n",
    "        \n",
    "        # Call the Cross_validated MSE values for each Strain Gauge\n",
    "        MSE_test = np.mean((y_obs_test-y_pred_test)**2) # Cross validation mse calucluated\n",
    "        MSE_test = \"{:.2E}\".format(MSE_test)\n",
    "        \n",
    "        # Call the evaluate_model function for each Strain Gauge\n",
    "        MAPE_test, _  , Ψ_test = evaluate_model(y_obs_test, y_pred_test, m)\n",
    "       \n",
    "        \n",
    "# POPULATE A DICTIONARY WITH RESULTS\n",
    "        model_results[variable] =  { 'MSE_train' : MSE_train,\n",
    "                           'MAPE_train' : MAPE_train,\n",
    "                           'AICc_train' : AICc_train,\n",
    "                           'Ψ_train' : Ψ_train,\n",
    "                           'MSE_test' : MSE_test, \n",
    "                           'MAPE_test' : MAPE_test,\n",
    "                           'Ψ_test' : Ψ_test}\n",
    "\n",
    "        \n",
    "calculate_model_performance(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelsFIVE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-a946aeb6dc78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_results\u001b[0m \u001b[1;31m# 600with13inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodelsFIVE\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SG9'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'modelsFIVE' is not defined"
     ]
    }
   ],
   "source": [
    "model_results # 600with13inputs\n",
    "modelsFIVE['SG9']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate a results table\n",
    "arrays = [np.hstack([ ['Training/validation']*4, ['Test']*4]), (['AICc','MSE', 'MAPE', 'Ψ(%)']*2)]\n",
    "\n",
    "columns = pd.MultiIndex.from_arrays(arrays, names=['', 'Model'])\n",
    "index=['SG1 - linear regression','SG1 - MLP',\n",
    "       'SG2 - linear regression','SG2 - MLP',\n",
    "       'SG3 - linear regression','SG3 - MLP',\n",
    "       'SG4 - linear regression','SG4 - MLP',\n",
    "       'SG5 - linear regression','SG5 - MLP',\n",
    "       'SG6 - linear regression','SG6 - MLP',\n",
    "       'SG7 - linear regression','SG7 - MLP',\n",
    "       'SG8 - linear regression','SG8 - MLP',\n",
    "       'SG9 - linear regression','SG9 - MLP',\n",
    "      ]\n",
    "\n",
    "results =pd.DataFrame(np.zeros((18,8)),columns=columns,index= index)\n",
    "\n",
    "\n",
    "#SG1\n",
    "results.loc['SG1 - linear regression'] = ['-703.5','3.41E+11','15.9','98.3','N/A','2.08E+11','35.1','106.9']\n",
    "results.loc['SG1 - MLP'] = [access_dictionary(model_results, 'SG1', 'AICc_train'),\n",
    "                              access_dictionary(model_results, 'SG1', 'MSE_train'),\n",
    "                              access_dictionary(model_results, 'SG1', 'MAPE_train'),\n",
    "                              access_dictionary(model_results, 'SG1', 'Ψ_train'),\n",
    "                              'N/A',\n",
    "                              access_dictionary(model_results, 'SG1', 'MSE_test'),\n",
    "                              access_dictionary(model_results, 'SG1', 'MAPE_test'),\n",
    "                              access_dictionary(model_results, 'SG1', 'Ψ_test')]\n",
    "\n",
    "\n",
    "#SG2\n",
    "results.loc['SG2 - linear regression'] = ['-911.7','6.87E+11','8.8','102.1','N/A','1.52E+12','35.0','119.1']\n",
    "results.loc['SG2 - MLP'] = [access_dictionary(model_results, 'SG2', 'AICc_train'),access_dictionary(model_results, 'SG2', 'MSE_train'),access_dictionary(model_results, 'SG2', 'MAPE_train'),access_dictionary(model_results, 'SG2', 'Ψ_train'),'N/A',access_dictionary(model_results, 'SG2', 'MSE_test'),access_dictionary(model_results, 'SG2', 'MAPE_test'),access_dictionary(model_results, 'SG2', 'Ψ_test')]\n",
    "\n",
    "#SG3\n",
    "results.loc['SG3 - linear regression'] = ['-464.9','2.00E+11','27.1','103.0','N/A','1.26E+11','41.5','99.5']\n",
    "results.loc['SG3 - MLP'] = [access_dictionary(model_results, 'SG3', 'AICc_train'),access_dictionary(model_results, 'SG3', 'MSE_train'),access_dictionary(model_results, 'SG3', 'MAPE_train'),access_dictionary(model_results, 'SG3', 'Ψ_train'),'N/A',access_dictionary(model_results, 'SG3', 'MSE_test'),access_dictionary(model_results, 'SG3', 'MAPE_test'),access_dictionary(model_results, 'SG3', 'Ψ_test')]\n",
    "\n",
    "\n",
    "#SG4\n",
    "results.loc['SG4 - linear regression'] = ['-615.1','3.48E+10','22.2','97.6','N/A','7.01E+10','32.4','104.5']\n",
    "results.loc['SG4 - MLP'] = [access_dictionary(model_results, 'SG4', 'AICc_train'),access_dictionary(model_results, 'SG4', 'MSE_train'),access_dictionary(model_results, 'SG4', 'MAPE_train'),access_dictionary(model_results, 'SG4', 'Ψ_train'),'N/A',access_dictionary(model_results, 'SG3', 'MSE_test'),access_dictionary(model_results, 'SG4', 'MAPE_test'),access_dictionary(model_results, 'SG4', 'Ψ_test')]\n",
    "\n",
    "\n",
    "#SG5\n",
    "results.loc['SG5 - linear regression'] = ['-799.3','1.76E+10','13.5','99.2','N/A','5.05E+11','45.1','125.2']\n",
    "results.loc['SG5 - MLP'] = [access_dictionary(model_results, 'SG5', 'AICc_train'),access_dictionary(model_results, 'SG5', 'MSE_train'),access_dictionary(model_results, 'SG5', 'MAPE_train'),access_dictionary(model_results, 'SG5', 'Ψ_train'),'N/A',access_dictionary(model_results, 'SG5', 'MSE_test'),access_dictionary(model_results, 'SG5', 'MAPE_test'),access_dictionary(model_results, 'SG5', 'Ψ_test')]\n",
    "\n",
    "\n",
    "#SG6\n",
    "results.loc['SG6 - linear regression'] = ['-641.2','3.17E+11','20.4','97.5','N/A','5.00E+11','36.8','111.7']\n",
    "results.loc['SG6 - MLP'] = [access_dictionary(model_results, 'SG6', 'AICc_train'),access_dictionary(model_results, 'SG6', 'MSE_train'),access_dictionary(model_results, 'SG6', 'MAPE_train'),access_dictionary(model_results, 'SG6', 'Ψ_train'),'N/A',access_dictionary(model_results, 'SG6', 'MSE_test'),access_dictionary(model_results, 'SG6', 'MAPE_test'),access_dictionary(model_results, 'SG6', 'Ψ_test')]\n",
    "\n",
    "\n",
    "#SG7\n",
    "results.loc['SG7 - linear regression'] = ['-622.2','3.54E+11','22.1','97.2','N/A','3.70E+11','54.6','122.0']\n",
    "results.loc['SG7 - MLP'] = [access_dictionary(model_results, 'SG7', 'AICc_train'),access_dictionary(model_results, 'SG7', 'MSE_train'),access_dictionary(model_results, 'SG7', 'MAPE_train'),access_dictionary(model_results, 'SG7', 'Ψ_train'),'N/A',access_dictionary(model_results, 'SG7', 'MSE_test'),access_dictionary(model_results, 'SG7', 'MAPE_test'),access_dictionary(model_results, 'SG7', 'Ψ_test')]\n",
    "\n",
    "\n",
    "#SG8\n",
    "results.loc['SG8 - linear regression'] = ['-900.2','1.31E+12','10.0','99.9','N/A','4.05E+12','38.6','116.7']\n",
    "results.loc['SG8 - MLP'] = [access_dictionary(model_results, 'SG8', 'AICc_train'),access_dictionary(model_results, 'SG8', 'MSE_train'),access_dictionary(model_results, 'SG8', 'MAPE_train'),access_dictionary(model_results, 'SG8', 'Ψ_train'),'N/A',access_dictionary(model_results, 'SG8', 'MSE_test'),access_dictionary(model_results, 'SG8', 'MAPE_test'),access_dictionary(model_results, 'SG8', 'Ψ_test')]\n",
    "\n",
    "#SG9\n",
    "results.loc['SG9 - linear regression'] = ['-795.9','1.08E+11','13.2','98.9','N/A','1.43E+10','32.9','107.3']\n",
    "results.loc['SG9 - MLP'] = [access_dictionary(model_results, 'SG9', 'AICc_train'),access_dictionary(model_results, 'SG9', 'MSE_train'),access_dictionary(model_results, 'SG9', 'MAPE_train'),access_dictionary(model_results, 'SG9', 'Ψ_train'),'N/A',access_dictionary(model_results, 'SG9', 'MSE_test'),access_dictionary(model_results, 'SG9', 'MAPE_test'),access_dictionary(model_results, 'SG9', 'Ψ_test')]\n",
    "results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "handlerId": "lineChart",
      "keyFields": "Strain Gauge",
      "legend": "true",
      "lineChartType": "grouped",
      "logx": "true",
      "logy": "true",
      "rowCount": "500",
      "valueFields": "AICc_Training_Isaac,AICc_Training_David"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Plotting AICc\n",
    "Index = ['SG1','SG2','SG3','SG4','SG5','SG6','SG7','SG8','SG9']\n",
    "AICc_Training_Isaac = [-703.5, -911.7, -464.9, -615.1, -799.3, -641.2, -622.2, -900.2, -795.9]\n",
    "AICc_Training_David = [access_dictionary(model_results, 'SG1', 'AICc_train'),access_dictionary(model_results, 'SG2', 'AICc_train'),access_dictionary(model_results, 'SG3', 'AICc_train'),access_dictionary(model_results, 'SG4', 'AICc_train'), access_dictionary(model_results, 'SG5', 'AICc_train'), access_dictionary(model_results, 'SG6', 'AICc_train'), access_dictionary(model_results, 'SG7', 'AICc_train'), access_dictionary(model_results, 'SG8', 'AICc_train'), access_dictionary(model_results, 'SG9', 'AICc_train')]\n",
    "AICc_Training_David = list(map(float, AICc_Training_David))\n",
    "\n",
    "# Plotting MSE - Training/Test\n",
    "MSE_Training_Isaac = [3.41E+11,6.87E+11,2.00E+11, 3.48E+10,1.76E+10,3.17E+11,3.54E+11,1.31E+12,1.08E+11]\n",
    "MSE__Training_David = [access_dictionary(model_results, 'SG1', 'MSE_train'),access_dictionary(model_results, 'SG2', 'MSE_train'),access_dictionary(model_results, 'SG3', 'MSE_train'),access_dictionary(model_results, 'SG4', 'MSE_train'),access_dictionary(model_results, 'SG5', 'MSE_train'),access_dictionary(model_results, 'SG6', 'MSE_train'),access_dictionary(model_results, 'SG7', 'MSE_train'),access_dictionary(model_results, 'SG8', 'MSE_train'),access_dictionary(model_results, 'SG9', 'MSE_train')]\n",
    "MSE__Training_David = list(map(float, MSE__Training_David))\n",
    "\n",
    "MSE_Test_Isaac = [2.08E+11, 1.52E+12, 1.26E+11, 7.01E+10, 5.05E+11, 5.00E+11, 3.70E+11, 4.05E+12, 1.43E+10]\n",
    "MSE__Test_David = [access_dictionary(model_results, 'SG1', 'MSE_test'),access_dictionary(model_results, 'SG2', 'MSE_test'),access_dictionary(model_results, 'SG3', 'MSE_test'),access_dictionary(model_results, 'SG4', 'MSE_test'),access_dictionary(model_results, 'SG5', 'MSE_test'),access_dictionary(model_results, 'SG6', 'MSE_test'),access_dictionary(model_results, 'SG7', 'MSE_test'),access_dictionary(model_results, 'SG8', 'MSE_test'),access_dictionary(model_results, 'SG9', 'MSE_test')]\n",
    "MSE__Test_David = list(map(float, MSE__Test_David))\n",
    "\n",
    "# Plotting MAPE - Training/Test\n",
    "MAPE_Training_Isaac = [15.9, 8.8, 27.1, 22.2, 13.5, 20.4, 22.1, 10.0z, 13.2]\n",
    "MAPE__Training_David = [access_dictionary(model_results, 'SG1', 'MAPE_train'),access_dictionary(model_results, 'SG2', 'MAPE_train'),access_dictionary(model_results, 'SG3', 'MAPE_train'),access_dictionary(model_results, 'SG4', 'MAPE_train'),access_dictionary(model_results, 'SG5', 'MAPE_train'),access_dictionary(model_results, 'SG6', 'MAPE_train'),access_dictionary(model_results, 'SG7', 'MAPE_train'),access_dictionary(model_results, 'SG8', 'MAPE_train'),access_dictionary(model_results, 'SG9', 'MAPE_train')]\n",
    "MAPE__Training_David = list(map(float, MAPE__Training_David))\n",
    "\n",
    "MAPE_Test_Isaac = [35.1, 35.0, 41.5, 32.4, 45.1, 36.8, 54.6, 38.6, 32.9]\n",
    "MAPE__Test_David = [access_dictionary(model_results, 'SG1', 'MAPE_test'),access_dictionary(model_results, 'SG2', 'MAPE_test'),access_dictionary(model_results, 'SG3', 'MAPE_test'),access_dictionary(model_results, 'SG4', 'MAPE_test'),access_dictionary(model_results, 'SG5', 'MAPE_test'),access_dictionary(model_results, 'SG6', 'MAPE_test'),access_dictionary(model_results, 'SG7', 'MAPE_test'),access_dictionary(model_results, 'SG8', 'MAPE_test'),access_dictionary(model_results, 'SG9', 'MAPE_test')]\n",
    "MAPE__Test_David = list(map(float, MAPE__Test_David))\n",
    "\n",
    "# Plotting Ψ(%) - Training/Test\n",
    "Ψ_Training_Isaac = [98.3, 102.1, 103.0, 97.6, 99.2, 97.5, 97.2, 99.9, 98.9]\n",
    "Ψ__Training_David = [access_dictionary(model_results, 'SG1', 'Ψ_train'),access_dictionary(model_results, 'SG2', 'Ψ_train'),access_dictionary(model_results, 'SG3', 'Ψ_train'),access_dictionary(model_results, 'SG4', 'Ψ_train'),access_dictionary(model_results, 'SG5', 'Ψ_train'),access_dictionary(model_results, 'SG6', 'Ψ_train'),access_dictionary(model_results, 'SG7', 'Ψ_train'),access_dictionary(model_results, 'SG8', 'Ψ_train'),access_dictionary(model_results, 'SG9', 'Ψ_train')]\n",
    "Ψ__Training_David = list(map(float, Ψ__Training_David))\n",
    "\n",
    "Ψ_Test_Isaac = [106.9, 119.1, 99.5, 104.5, 125.2, 111.7, 122.0, 116.7, 107.3]\n",
    "Ψ__Test_David = [access_dictionary(model_results, 'SG1', 'Ψ_test'),access_dictionary(model_results, 'SG2', 'Ψ_test'),access_dictionary(model_results, 'SG3', 'Ψ_test'),access_dictionary(model_results, 'SG4', 'Ψ_test'),access_dictionary(model_results, 'SG5', 'Ψ_test'),access_dictionary(model_results, 'SG6', 'Ψ_test'),access_dictionary(model_results, 'SG7', 'Ψ_test'),access_dictionary(model_results, 'SG8', 'Ψ_test'),access_dictionary(model_results, 'SG9', 'Ψ_test')]\n",
    "Ψ__Test_David = list(map(float, Ψ__Test_David))\n",
    "\n",
    "AICc_data = pd.DataFrame(\n",
    "    { 'Strain Gauge' : Index,\n",
    "     'AICc - Training Error (multiple linear regression)' : AICc_Training_Isaac,\n",
    "     'AICc - Training Error (MLP)': AICc_Training_David})\n",
    "\n",
    "MSE_data = pd.DataFrame(\n",
    "    { 'Strain Gauge' : Index,\n",
    "     'MSE - Training Error (multiple linear regression)' : MSE_Training_Isaac,\n",
    "     'MSE - Test Error (multiple linear regression)' : MSE_Test_Isaac,\n",
    "     'MSE - Training Error (MLP)': MSE__Training_David,\n",
    "     'MSE - Test Error (MLP)': MSE__Test_David})\n",
    "\n",
    "MAPE_data = pd.DataFrame(\n",
    "    { 'Strain Gauge' : Index,\n",
    "     'MAPE - Training Error (multiple linear regression)' : MAPE_Training_Isaac,\n",
    "     'MAPE - Test Error (multiple linear regression)' : MAPE_Test_Isaac,\n",
    "     'MAPE - Training Error (MLP)': MAPE__Training_David,\n",
    "     'MAPE - Test Error (MLP)': MAPE__Test_David})\n",
    "\n",
    "Ψ_data = pd.DataFrame(\n",
    "    { 'Strain Gauge' : Index,\n",
    "    ' Ψ - Training Error (multiple linear regression)' : Ψ_Training_Isaac,\n",
    "     'Ψ - Test Error (multiple linear regression)' : Ψ_Test_Isaac,\n",
    "     'Ψ - Training Error (MLP)': Ψ__Training_David,\n",
    "     'Ψ - Test Error (MLP)': Ψ__Test_David})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "AVG",
      "chartsize": "100",
      "handlerId": "lineChart",
      "keyFields": "Strain Gauge",
      "legend": "true",
      "lineChartType": "grouped",
      "logx": "false",
      "logy": "false",
      "rowCount": "1000",
      "timeseries": "false",
      "valueFields": "MSE - Training Error (MLP),MSE - Test Error (MLP),MSE - Training Error (multiple linear regression),MSE - Test Error (multiple linear regression)"
     }
    }
   },
   "outputs": [],
   "source": [
    "import pixiedust\n",
    "display(MSE_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pixiedust": {
     "displayParams": {
      "chartsize": "53",
      "handlerId": "scatterPlot",
      "keyFields": "predict",
      "kind": "reg",
      "mpld3": "true",
      "rendererId": "seaborn",
      "rowCount": "500",
      "valueFields": "SG9"
     }
    }
   },
   "outputs": [],
   "source": [
    "Strain_gauge = 'SG9'\n",
    "y_obs_test = test_frame[Strain_gauge].as_data_frame(use_pandas=True)\n",
    "y_pred_test = models[Strain_gauge].predict(test_frame).as_data_frame(use_pandas=True)\n",
    "combined =  pd.concat([y_obs_test,y_pred_test], join='inner', axis=1)\n",
    "import pixiedust\n",
    "display(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install lolviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandoc in c:\\programdata\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: ply in c:\\programdata\\anaconda3\\lib\\site-packages (from pandoc)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandoc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
